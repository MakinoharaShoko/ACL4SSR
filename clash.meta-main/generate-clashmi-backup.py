#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pyyaml", "requests"]
# ///
"""
ç”Ÿæˆ Clash Mi å…¼å®¹çš„å¤‡ä»½ zipï¼ˆè‡ªåŒ…å«ï¼Œæ— éœ€ patchï¼‰

æ ¸å¿ƒé€»è¾‘:
  1. ä» proxy-providers æœ¬åœ°ç¼“å­˜è¯»å–èŠ‚ç‚¹
  2. base64 æ ¼å¼çš„è®¢é˜…è§£ææˆé™æ€ proxies
  3. ç”¨ proxy-groups çš„ filter/exclude-filter æ­£åˆ™å±•å¼€æˆé™æ€ proxies åˆ—è¡¨
  4. è§„åˆ™è½¬æ¢æˆ mrs äºŒè¿›åˆ¶æ ¼å¼ï¼ˆçœ 10x å†…å­˜ï¼‰
  5. ç”Ÿæˆå¯ç›´æ¥å¯¼å…¥çš„ backup.zip

æ³¨æ„: Clash Mi å¯¼å…¥æ˜¯å®Œå…¨æ›¿æ¢ï¼Œä¸æ˜¯åˆå¹¶ï¼
"""

import json
import yaml
import re
import base64
import zipfile
import urllib.parse
import subprocess
import tempfile
import shutil
from pathlib import Path
from datetime import datetime


def load_config(path: Path) -> dict:
    with open(path, encoding="utf-8") as f:
        return yaml.safe_load(f)


def expand_rule_providers(rules: list, rule_providers: dict, ruleset_dir: Path, skip_providers: set = None) -> list:
    """å±•å¼€ RULE-SET è§„åˆ™ä¸ºå†…è”è§„åˆ™ï¼Œskip_providers é‡Œçš„ä¸å±•å¼€ï¼ˆç”¨äºè·³è¿‡è¶…å¤§è§„åˆ™é›†ï¼‰"""
    skip_providers = skip_providers or set()
    expanded = []
    
    for rule in rules:
        # æ£€æŸ¥æ˜¯å¦ä¸º RULE-SET è§„åˆ™ï¼ˆå…è®¸ RULE-SET å’Œé€—å·ä¹‹é—´æœ‰ç©ºæ ¼ï¼‰
        rule_stripped = rule.strip()
        if not rule_stripped.upper().startswith("RULE-SET"):
            expanded.append(rule)
            continue
        
        # è§£æ RULE-SET,provider_name,target[,NO-RESOLVE]
        parts = [p.strip() for p in rule.split(",")]  # å»é™¤ç©ºæ ¼
        provider_name = parts[1]
        target = parts[2]
        no_resolve = ",NO-RESOLVE" if len(parts) > 3 and "NO-RESOLVE" in parts[3].upper() else ""
        
        # è·³è¿‡æŒ‡å®šçš„è¶…å¤§è§„åˆ™é›†
        if provider_name in skip_providers:
            print(f"â­ï¸  è·³è¿‡ {provider_name}")
            continue
        
        # æŸ¥æ‰¾ provider é…ç½®
        provider = rule_providers.get(provider_name)
        if not provider:
            print(f"âš ï¸  æœªçŸ¥ provider: {provider_name}")
            continue
        
        # ç¡®å®šç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_path = provider.get("path", "")
        if cache_path:
            cache_file = ruleset_dir.parent / cache_path.lstrip("./")
        else:
            cache_file = ruleset_dir / f"{provider_name}.yaml"
        
        if not cache_file.exists():
            print(f"âš ï¸  ç¼“å­˜ä¸å­˜åœ¨: {cache_file}")
            continue
        
        # è¯»å–è§„åˆ™
        try:
            content = cache_file.read_text(encoding="utf-8")
            behavior = provider.get("behavior", "domain")
            fmt = provider.get("format", "yaml")
            
            if fmt == "text" or not content.strip().startswith("payload:"):
                # çº¯æ–‡æœ¬æ ¼å¼ï¼šæ¯è¡Œä¸€æ¡è§„åˆ™
                lines = [l.strip() for l in content.split("\n") if l.strip() and not l.startswith("#")]
                
                for line in lines:
                    if "," in line:
                        # å·²æœ‰è§„åˆ™æ ¼å¼ï¼šDOMAIN,xxx æˆ– DOMAIN-SUFFIX,xxx
                        expanded.append(f"{line},{target}{no_resolve}")
                    else:
                        # çº¯åŸŸå
                        expanded.append(f"DOMAIN-SUFFIX,{line},{target}{no_resolve}")
                print(f"ğŸ“‹ {provider_name}: {len(lines)} æ¡è§„åˆ™ â†’ {target}")
            else:
                # YAML payload æ ¼å¼
                data = yaml.safe_load(content)
                payload = data.get("payload", [])
                
                for item in payload:
                    if behavior == "domain":
                        expanded.append(f"DOMAIN-SUFFIX,{item},{target}{no_resolve}")
                    elif behavior == "ipcidr":
                        if ":" in item:
                            expanded.append(f"IP-CIDR6,{item},{target}{no_resolve}")
                        else:
                            expanded.append(f"IP-CIDR,{item},{target}{no_resolve}")
                    elif behavior == "classical":
                        if "," in item:
                            rule_parts = item.split(",")
                            rule_parts[-1] = target
                            expanded.append(",".join(rule_parts) + no_resolve)
                        else:
                            expanded.append(f"{item},{target}{no_resolve}")
                
                print(f"ğŸ“‹ {provider_name}: {len(payload)} æ¡è§„åˆ™ â†’ {target}")
        except Exception as e:
            print(f"âš ï¸  è§£æå¤±è´¥ {cache_file}: {e}")
            continue
    
    return expanded


def convert_to_mrs(yaml_path: Path, behavior: str, output_path: Path) -> tuple[bool, str]:
    """å°†è§„åˆ™æ–‡ä»¶è½¬æ¢ä¸º mrs äºŒè¿›åˆ¶æ ¼å¼
    
    Args:
        yaml_path: æºè§„åˆ™æ–‡ä»¶è·¯å¾„
        behavior: åŸå§‹ behavior (domain/ipcidr/classical)
        output_path: mrs è¾“å‡ºè·¯å¾„
    
    Returns:
        (æˆåŠŸä¸å¦, å®é™…ä½¿ç”¨çš„ behavior)
    
    mrs åªæ”¯æŒ domain å’Œ ipcidrï¼Œclassical è§„åˆ™ä¼šè¢«è§£æï¼š
    - DOMAIN,xxx â†’ xxx (ç²¾ç¡®åŒ¹é…)
    - DOMAIN-SUFFIX,xxx â†’ .xxx (åç¼€åŒ¹é…ï¼ŒåŠ ç‚¹å·)
    - DOMAIN-KEYWORD,xxx â†’ +xxx (å…³é”®å­—åŒ¹é…ï¼ŒåŠ +å·)
    """
    try:
        content = yaml_path.read_text(encoding="utf-8")
        
        # è§£æè§„åˆ™
        if content.strip().startswith("payload:"):
            data = yaml.safe_load(content)
            lines = data.get("payload", [])
        else:
            lines = [l.strip() for l in content.split("\n") if l.strip() and not l.startswith("#")]
        
        clean_lines = []
        has_process_rules = False
        
        for line in lines:
            upper = line.upper()
            
            # hosts æ–‡ä»¶æ ¼å¼
            if line.startswith(("0.0.0.0", "127.0.0.1")):
                parts = line.split()
                if len(parts) >= 2:
                    clean_lines.append(f".{parts[1]}")  # ä½œä¸ºåç¼€åŒ¹é…
            # PROCESS è§„åˆ™ - iOS ä¸æ”¯æŒ
            elif upper.startswith("PROCESS-"):
                has_process_rules = True
            # DOMAIN ç²¾ç¡®åŒ¹é…
            elif upper.startswith("DOMAIN,"):
                domain = line.split(",")[1].strip()
                clean_lines.append(domain)
            # DOMAIN-SUFFIX åç¼€åŒ¹é…
            elif upper.startswith("DOMAIN-SUFFIX,"):
                domain = line.split(",")[1].strip()
                clean_lines.append(f".{domain}")  # åŠ ç‚¹å·è¡¨ç¤ºåç¼€
            # DOMAIN-KEYWORD å…³é”®å­—åŒ¹é…
            elif upper.startswith("DOMAIN-KEYWORD,"):
                keyword = line.split(",")[1].strip()
                clean_lines.append(f"+{keyword}")  # åŠ +å·è¡¨ç¤ºå…³é”®å­—
            # IP ç±»è§„åˆ™ - åªåœ¨ ipcidr behavior æ—¶å¤„ç†
            elif upper.startswith(("IP-CIDR,", "IP-CIDR6,")):
                if behavior == "ipcidr":
                    clean_lines.append(line.split(",")[1].strip())
            # å…¶ä»–ä¸æ”¯æŒçš„è§„åˆ™ç±»å‹
            elif upper.startswith(("SRC-IP", "DST-PORT", "SRC-PORT", "GEOIP")):
                continue
            else:
                # æ™®é€šåŸŸåè¡Œï¼ˆå¯èƒ½å¸¦å¼•å·ï¼‰
                clean = line.strip("'\"")
                if clean:
                    clean_lines.append(f".{clean}" if not clean.startswith((".", "+", "*")) else clean)
        
        if not clean_lines:
            if has_process_rules:
                print(f"   â„¹ï¸  è§„åˆ™å…¨æ˜¯ PROCESS-NAMEï¼ˆiOS ä¸æ”¯æŒï¼‰")
            return False, ""
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(mode="w", suffix=".txt", delete=False, encoding="utf-8") as f:
            f.write("\n".join(clean_lines))
            temp_txt = Path(f.name)
        
        # ç¡®å®šè½¬æ¢ behavior
        convert_behavior = "ipcidr" if behavior == "ipcidr" else "domain"
        
        # è°ƒç”¨ mihomo è½¬æ¢
        result = subprocess.run(
            ["nix-shell", "-p", "mihomo", "--run", f"mihomo convert-ruleset {convert_behavior} text {temp_txt} {output_path}"],
            capture_output=True, text=True, timeout=60
        )
        
        temp_txt.unlink()
        
        if output_path.exists() and output_path.stat().st_size > 0:
            return True, convert_behavior
        return False, ""
    except Exception as e:
        print(f"âš ï¸  mrs è½¬æ¢å¤±è´¥: {e}")
        return False, ""


def parse_ss_link(link: str) -> dict | None:
    """è§£æ ss:// é“¾æ¥ä¸º proxy é…ç½®"""
    try:
        # ss://base64#name æˆ– ss://method:password@server:port#name
        if "#" in link:
            main, name = link[5:].split("#", 1)
            name = urllib.parse.unquote(name)
        else:
            main, name = link[5:], "SS"
        
        # å°è¯• base64 è§£ç 
        try:
            decoded = base64.b64decode(main + "===").decode()
            # method:password@server:port
            if "@" in decoded:
                method_pass, server_port = decoded.rsplit("@", 1)
                method, password = method_pass.split(":", 1)
                server, port = server_port.rsplit(":", 1)
            else:
                return None
        except:
            # é base64 æ ¼å¼: method:password@server:port
            if "@" in main:
                method_pass, server_port = main.rsplit("@", 1)
                method, password = method_pass.split(":", 1)
                server, port = server_port.rsplit(":", 1)
            else:
                return None
        
        return {
            "name": name,
            "type": "ss",
            "server": server,
            "port": int(port),
            "cipher": method,
            "password": password,
            "udp": True
        }
    except:
        return None


def parse_vmess_link(link: str) -> dict | None:
    """è§£æ vmess:// é“¾æ¥ä¸º proxy é…ç½®"""
    try:
        data = json.loads(base64.b64decode(link[8:] + "===").decode())
        return {
            "name": data.get("ps", "VMess"),
            "type": "vmess",
            "server": data.get("add", ""),
            "port": int(data.get("port", 443)),
            "uuid": data.get("id", ""),
            "alterId": int(data.get("aid", 0)),
            "cipher": data.get("type", "auto") if data.get("type") != "none" else "auto",
            "network": data.get("net", "tcp"),
            "tls": data.get("tls") == "tls",
            "udp": True
        }
    except:
        return None


def parse_subscription(content: str) -> tuple[list[dict], list[str], bool]:
    """
    è§£æè®¢é˜…å†…å®¹ï¼Œè¿”å› (proxiesåˆ—è¡¨, èŠ‚ç‚¹ååˆ—è¡¨, æ˜¯å¦YAMLæ ¼å¼)
    """
    proxies = []
    names = []
    
    # å°è¯•è§£æä¸º YAML
    try:
        data = yaml.safe_load(content)
        if isinstance(data, dict) and "proxies" in data:
            proxies = data["proxies"]
            names = [n["name"] for n in proxies if "name" in n]
            return proxies, names, True
    except:
        pass
    
    # å°è¯•è§£æ base64 ç¼–ç çš„é“¾æ¥åˆ—è¡¨
    try:
        decoded = base64.b64decode(content.strip() + "===").decode("utf-8")
        for line in decoded.strip().split("\n"):
            line = line.strip()
            if not line:
                continue
            
            proxy = None
            if line.startswith("ss://"):
                proxy = parse_ss_link(line)
            elif line.startswith("vmess://"):
                proxy = parse_vmess_link(line)
            # TODO: vless, trojan ç­‰
            
            if proxy:
                proxies.append(proxy)
                names.append(proxy["name"])
    except:
        pass
    
    return proxies, names, False


def load_provider_nodes(providers: dict, cache_dir: Path) -> tuple[dict[str, list[str]], list[dict], list[str]]:
    """
    ä»æœ¬åœ°ç¼“å­˜åŠ è½½æ‰€æœ‰ provider çš„èŠ‚ç‚¹
    è¿”å›: (èŠ‚ç‚¹åæ˜ å°„, éœ€è¦é™æ€å†™å…¥çš„proxies, æ”¯æŒYAMLçš„providerååˆ—è¡¨)
    """
    node_names = {}
    static_proxies = []
    yaml_providers = []
    
    for name, p in providers.items():
        cache_file = cache_dir / f"{name}.yaml"
        if not cache_file.exists():
            print(f"âš ï¸  ç¼“å­˜ä¸å­˜åœ¨: {cache_file}")
            continue
        
        content = cache_file.read_text(encoding="utf-8")
        proxies, names, is_yaml = parse_subscription(content)
        
        node_names[name] = names
        static_proxies.extend(proxies)  # æ‰€æœ‰èŠ‚ç‚¹éƒ½åŠ å…¥é™æ€åˆ—è¡¨
        
        if is_yaml:
            yaml_providers.append(name)
            print(f"ğŸ“¦ {name}: {len(names)} ä¸ªèŠ‚ç‚¹ (YAML)")
        else:
            print(f"ğŸ“¦ {name}: {len(names)} ä¸ªèŠ‚ç‚¹ (base64â†’é™æ€)")
    
    return node_names, static_proxies, yaml_providers


def expand_proxy_group(group: dict, all_nodes: dict[str, list[str]]) -> dict:
    """å±•å¼€ use/filter ä¸ºé™æ€ proxies åˆ—è¡¨"""
    # æ¸…ç† YAML é”šç‚¹æ®‹ç•™
    result = {k: v for k, v in group.items() if not k.startswith("<<")}
    
    # å¦‚æœæ²¡æœ‰ useï¼Œç›´æ¥è¿”å›
    if "use" not in result:
        return result
    
    # æ”¶é›†æ‰€æœ‰å€™é€‰èŠ‚ç‚¹
    candidates = []
    for provider_name in result.get("use", []):
        candidates.extend(all_nodes.get(provider_name, []))
    
    # åº”ç”¨ filter
    if flt := result.get("filter"):
        pattern = re.compile(flt, re.IGNORECASE)
        candidates = [n for n in candidates if pattern.search(n)]
    
    # åº”ç”¨ exclude-filter
    if exc := result.get("exclude-filter"):
        pattern = re.compile(exc, re.IGNORECASE)
        candidates = [n for n in candidates if not pattern.search(n)]
    
    # åˆå¹¶ç°æœ‰ proxiesï¼ˆå¦‚ DIRECTï¼‰
    existing = result.get("proxies", [])
    result["proxies"] = existing + candidates
    
    # ç§»é™¤åŠ¨æ€å­—æ®µ
    for key in ["use", "filter", "exclude-filter"]:
        result.pop(key, None)
    
    return result


def extract_rules(config: dict, expanded_groups: list) -> dict:
    """æå– diversion_templateï¼ˆApp å±‚æ¨¡æ¿ï¼Œéè§„åˆ™æœ¬èº«ï¼‰"""
    # proxygroup-templates ç»™ App UI ç”¨
    proxygroup_templates = []
    for g in expanded_groups:
        template = {
            "name": g.get("name", ""),
            "type": g.get("type", "select"),
            "proxies": g.get("proxies", []),
        }
        if icon := g.get("icon"):
            template["icon"] = icon
        proxygroup_templates.append(template)
    
    return {
        "rule-providers": [],  # è§„åˆ™å·²åœ¨ local_config é‡Œ
        "rule-templates": [],  # ä¸ä½¿ç”¨æ¨¡æ¿
        "proxygroup-templates": proxygroup_templates
    }


def extract_proxies(config: dict) -> list:
    """æå–é™æ€ proxiesï¼ˆå¦‚é˜¿é‡Œäº‘æ­å·ï¼‰"""
    return config.get("proxies", [])


# è§„åˆ™å¤§å°åˆ†ç±»ï¼ˆæŒ‰æ¡æ•°ï¼‰
RULE_SIZE_LIMITS = {
    "lite": 10000,    # è½»é‡ç‰ˆï¼š<1ä¸‡æ¡
    "medium": 50000,  # ä¸­ç­‰ç‰ˆï¼š<5ä¸‡æ¡
    "full": None,     # å®Œæ•´ç‰ˆï¼šæ— é™åˆ¶
}


def generate_backup(config: dict, config_dir: Path, cache_dir: Path, icloud: Path, 
                    node_names: set, all_proxies: list, expanded_groups: list):
    """ç”Ÿæˆ Clash Mi å¤‡ä»½ï¼Œè‡ªåŠ¨åˆå¹¶æ‰€æœ‰ REJECT è§„åˆ™é›†"""
    
    print(f"\n{'='*50}")
    print(f"ğŸ“¦ ç”Ÿæˆ Clash Mi å¤‡ä»½ï¼ˆåˆå¹¶ REJECT è§„åˆ™ï¼‰")
    print(f"{'='*50}")
    
    # å¤„ç† rules
    rules = []
    for r in config.get("rules", []):
        if not r.startswith(("PROCESS-NAME", "PROCESS-PATH")):
            rules.append(r)
    
    # è½¬æ¢ rule-providers
    ruleset_dir = config_dir / "ruleset"
    rule_providers_file = {}
    ruleset_files = {}
    tmp_mrs_dir = Path(tempfile.mkdtemp())
    skipped_providers = set()
    reject_rulesets_to_merge = {}  # REJECT è§„åˆ™é›†
    
    for name, provider in config.get("rule-providers", {}).items():
        cache_path = provider.get("path", "")
        if cache_path:
            cache_file = config_dir / cache_path.lstrip("./")
        else:
            cache_file = ruleset_dir / f"{name}.yaml"
        
        if not cache_file.exists():
            continue
        
        # å¦‚æœå·²ç»æ˜¯ mrs æ ¼å¼ï¼Œç›´æ¥å¤åˆ¶
        if provider.get("format") == "mrs":
            rule_providers_file[name] = {
                "type": "file",
                "behavior": provider.get("behavior", "domain"),
                "path": f"./profiles/ruleset/{name}.mrs",
                "format": "mrs",
            }
            ruleset_files[f"profiles/ruleset/{name}.mrs"] = cache_file.read_bytes()
            print(f"ğŸ“‹ {name}: mrs ç›´æ¥å¤åˆ¶ ({cache_file.stat().st_size//1024}KB)")
            continue
        
        # è®¡ç®—è§„åˆ™æ¡æ•°
        content = cache_file.read_text(encoding="utf-8")
        if content.strip().startswith("payload:"):
            data = yaml.safe_load(content)
            rule_count = len(data.get("payload", []))
        else:
            rule_count = len([l for l in content.split("\n") if l.strip() and not l.startswith("#")])
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ REJECT è§„åˆ™ï¼ˆéœ€è¦åˆå¹¶ï¼‰
        is_reject = False
        for r in config.get("rules", []):
            if f"RULE-SET,{name}," in r or f"RULE-SET,{name} " in r:
                if ",REJECT" in r.upper():
                    is_reject = True
                break
        
        if is_reject:
            reject_rulesets_to_merge[name] = (cache_file, provider.get("behavior", "domain"), rule_count)
            skipped_providers.add(name)
            print(f"ğŸ”„ æ”¶é›† {name}ï¼š{rule_count} æ¡ï¼ˆå¾…åˆå¹¶ï¼‰")
            continue
        
        behavior = provider.get("behavior", "domain")
        mrs_path = tmp_mrs_dir / f"{name}.mrs"
        success, actual_behavior = convert_to_mrs(cache_file, behavior, mrs_path)
        
        if success:
            rule_providers_file[name] = {
                "type": "file",
                "behavior": actual_behavior,
                "path": f"./profiles/ruleset/{name}.mrs",
                "format": "mrs",
            }
            ruleset_files[f"profiles/ruleset/{name}.mrs"] = mrs_path.read_bytes()
            
            original_size = cache_file.stat().st_size
            mrs_size = mrs_path.stat().st_size
            ratio = original_size / mrs_size if mrs_size > 0 else 0
            print(f"ğŸ“‹ {name}: {rule_count}æ¡ ({original_size//1024}KB â†’ {mrs_size//1024}KB, {ratio:.1f}x)")
        else:
            # æ£€æŸ¥æ˜¯å¦å…¨æ˜¯ PROCESS-NAME
            if content.strip().startswith("payload:"):
                rules_list = yaml.safe_load(content).get("payload", [])
            else:
                rules_list = [l.strip() for l in content.split("\n") if l.strip() and not l.startswith("#")]
            
            if all(r.upper().startswith("PROCESS-") for r in rules_list if r):
                skipped_providers.add(name)
                continue
    
    # åˆå¹¶ REJECT è§„åˆ™é›†
    if reject_rulesets_to_merge:
        print(f"\nğŸ”€ åˆå¹¶ {len(reject_rulesets_to_merge)} ä¸ª REJECT è§„åˆ™é›†...")
        merged_domains = set()
        
        for name, (file_path, behavior, count) in reject_rulesets_to_merge.items():
            content = file_path.read_text(encoding="utf-8")
            
            # è§£æè§„åˆ™
            if content.strip().startswith("payload:"):
                data = yaml.safe_load(content)
                lines = data.get("payload", [])
            else:
                lines = [l.strip() for l in content.split("\n") if l.strip() and not l.startswith("#")]
            
            before = len(merged_domains)
            for line in lines:
                upper = line.upper()
                if line.startswith(("0.0.0.0", "127.0.0.1")):
                    parts = line.split()
                    if len(parts) >= 2:
                        merged_domains.add(f".{parts[1].lower()}")
                elif upper.startswith("DOMAIN,"):
                    merged_domains.add(line.split(",")[1].strip().lower())
                elif upper.startswith("DOMAIN-SUFFIX,"):
                    merged_domains.add(f".{line.split(',')[1].strip().lower()}")
                elif upper.startswith("DOMAIN-KEYWORD,"):
                    merged_domains.add(f"+{line.split(',')[1].strip().lower()}")
                elif line.startswith("*."):
                    merged_domains.add(f".{line[2:].lower()}")
                elif not upper.startswith(("PROCESS-", "IP-", "SRC-", "DST-", "GEOIP")):
                    merged_domains.add(f".{line.lower()}")
            
            new = len(merged_domains) - before
            print(f"   + {name}: {count} æ¡ â†’ æ–°å¢ {new} æ¡")
        
        # è½¬æ¢åˆå¹¶ç»“æœä¸º mrs
        print(f"   = åˆå¹¶å»é‡å: {len(merged_domains)} æ¡")
        
        merged_txt = tmp_mrs_dir / "combined-reject.txt"
        merged_mrs = tmp_mrs_dir / "combined-reject.mrs"
        
        with open(merged_txt, "w") as f:
            f.write("\n".join(sorted(merged_domains)))
        
        result = subprocess.run([
            "nix-shell", "-p", "mihomo", "--run",
            f'mihomo convert-ruleset domain text "{merged_txt}" "{merged_mrs}"'
        ], capture_output=True, text=True, timeout=120)
        
        if merged_mrs.exists():
            rule_providers_file["combined-reject"] = {
                "type": "file",
                "behavior": "domain",
                "path": "./profiles/ruleset/combined-reject.mrs",
                "format": "mrs",
            }
            ruleset_files["profiles/ruleset/combined-reject.mrs"] = merged_mrs.read_bytes()
            
            mrs_size = merged_mrs.stat().st_size
            print(f"âœ… combined-reject: {len(merged_domains)}æ¡ â†’ {mrs_size//1024}KB mrs")
    
    # è¿‡æ»¤å’Œæ›¿æ¢ rules
    filtered_rules = []
    combined_rule_added = False
    merged_names = set(reject_rulesets_to_merge.keys())
    
    for r in rules:
        skip = False
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¢«åˆå¹¶çš„è§„åˆ™é›†
        for p in merged_names:
            if f"RULE-SET,{p}," in r or f"RULE-SET,{p} " in r or r.endswith(f"RULE-SET,{p}"):
                skip = True
                if not combined_rule_added and reject_rulesets_to_merge:
                    # ç”¨ combined-reject æ›¿ä»£ç¬¬ä¸€ä¸ªè¢«åˆå¹¶çš„è§„åˆ™
                    filtered_rules.append("RULE-SET,combined-reject,REJECT")
                    combined_rule_added = True
                break
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è¢«è·³è¿‡çš„è§„åˆ™é›†ï¼ˆéåˆå¹¶ï¼‰
        if not skip:
            for p in (skipped_providers - merged_names):
                if f"RULE-SET,{p}," in r or f"RULE-SET,{p} " in r or r.endswith(f"RULE-SET,{p}"):
                    skip = True
                    break
        
        if not skip:
            filtered_rules.append(r)
    
    shutil.rmtree(tmp_mrs_dir, ignore_errors=True)
    
    # ç”Ÿæˆé…ç½®
    static_yaml = {
        "proxies": all_proxies,
        "proxy-groups": expanded_groups,
        "rule-providers": rule_providers_file,
        "rules": filtered_rules,
    }
    static_content = yaml.dump(static_yaml, allow_unicode=True, sort_keys=False)
    
    profiles_data = {
        "current_id": "local_config.yaml",
        "profiles": [{
            "id": "local_config.yaml",
            "name": "æ¡Œé¢è¿ç§»é…ç½®",
            "url": "",
            "update_interval": 0,
            "enabled": True,
        }]
    }
    
    # ç”Ÿæˆ ZIP
    timestamp = datetime.now().strftime("%Y-%m-%d-%H%M")
    zip_path = icloud / f"ClashMi_{timestamp}.backup.zip"
    
    with zipfile.ZipFile(zip_path, "w", zipfile.ZIP_DEFLATED) as zf:
        zf.writestr("profiles.json", json.dumps(profiles_data, ensure_ascii=False, indent=2))
        zf.writestr("profile_patchs.json", json.dumps({"current_id": "", "profile_patchs": []}, indent=2))
        zf.writestr("diversion_template.json", json.dumps(extract_rules(config, expanded_groups), ensure_ascii=False, indent=2))
        zf.writestr("profiles/", "")
        zf.writestr("profiles/ruleset/", "")
        zf.writestr("profiles/local_config.yaml", static_content)
        for filepath, content in ruleset_files.items():
            zf.writestr(filepath, content)
    
    size_kb = zip_path.stat().st_size / 1024
    print(f"âœ… {zip_path.name} ({size_kb:.0f} KB)")
    print(f"   {len(rule_providers_file)} ä¸ªè§„åˆ™æ–‡ä»¶, {len(filtered_rules)} æ¡è§„åˆ™")
    return zip_path


def main():
    config_dir = Path(__file__).parent
    config_path = config_dir / "config.yaml"
    cache_dir = config_dir / "proxy_providers"
    
    if not config_path.exists():
        print(f"âŒ é…ç½®ä¸å­˜åœ¨: {config_path}")
        return 1
    
    config = load_config(config_path)
    
    # åŠ è½½æ‰€æœ‰ provider çš„èŠ‚ç‚¹
    node_names, static_proxies, _ = load_provider_nodes(
        config.get("proxy-providers", {}), cache_dir
    )
    
    # åˆå¹¶ config.yaml é‡Œçš„é™æ€ proxies
    all_proxies = config.get("proxies", []) + static_proxies
    
    # å±•å¼€ proxy-groups
    expanded_groups = []
    for g in config.get("proxy-groups", []):
        expanded = expand_proxy_group(g, node_names)
        expanded_groups.append(expanded)
        if "use" in g:
            print(f"ğŸ”„ {expanded['name']}: {len(expanded.get('proxies', []))} ä¸ªèŠ‚ç‚¹")
    
    # è¾“å‡ºåˆ° iCloud
    icloud = Path.home() / "Library/Mobile Documents/iCloud~com~nebula~clashmi"
    icloud.mkdir(parents=True, exist_ok=True)
    
    # æ¸…ç†æ—§å¤‡ä»½
    for f in icloud.glob("ClashMi_*.backup.zip"):
        f.unlink()
        print(f"ğŸ—‘ï¸  æ¸…ç†: {f.name}")
    
    print(f"\nâš ï¸  è·³è¿‡ PROCESS-NAME è§„åˆ™ï¼ˆiOS ä¸æ”¯æŒï¼‰")
    
    # ç”Ÿæˆå¤‡ä»½ï¼šè‡ªåŠ¨åˆå¹¶æ‰€æœ‰ REJECT è§„åˆ™é›†
    generate_backup(config, config_dir, cache_dir, icloud,
                   node_names, all_proxies, expanded_groups)


if __name__ == "__main__":
    exit(main() or 0)
